two use cases:
(1) as a dedicated library for GPU matrix programming with a subset of Armadillo-like functions.
(2) as a drop-in accelerator for the Armadillo library, offloading intensive computations to the GPU when possible.

TODO

timing results on various GPU cards for a subset of functions
- single precision
- double precision


investigate use of clBLAS axpy() for simple addition and subtraction of arrays

handle 64 bit integers on GPU side
if bcoot_64bit_word is enabled, use 64 bit integers on GPU side (if available)
this will require making 2 versions of each kernel

for functions such as extracting diagonals, there is no point in trying to do this in parallel
can implement a kernel which is instantiated only once (ie. global_work_size = 1)
which does a for-loop internally.

overall summation:
split the matrix into several even chunks.  what happens if the matrix size is odd?
or, sum each column

need to do an experiment to see if parallelisation helps
approach 1: sum each column (colwise-sum) then sum the sums (vector sum, no parallelisation)
vector sum kernel would do a for loop internally - ie. global_work_size is 1
approach 2: call vector sum directly


Mat::Mat(Mat&)        [done]
Mat::operator=(Mat&)  [done]

Mat::Mat(Mat&&)       [done]
Mat::operator=(Mat&&) [done]

need arrayops so code can be shared with Cube [done]
Mat::operator+=(Mat&) [done]
Mat::operator-=(Mat&) [done]
Mat::operator*=(Mat&) [done]
Mat::operator/=(Mat&) [done]

need arrayops so code can be shared with Cube [done]
Mat::operator= (scalar) [done]
Mat::operator+=(scalar) [done]
Mat::operator-=(scalar) [done]
Mat::operator*=(scalar) [done]
Mat::operator/=(scalar) [done]


TODO: only a small subset of element-wise operations has been implemented;
TODO: need to implement many unary math and trig functions
Mat::operator= (eOp) [done]
Mat::operator+=(eOp) [done]
Mat::operator-=(eOp) [done]
Mat::operator*=(eOp) 
Mat::operator/=(eOp) [done]

Mat::operator= (eGlue) [done]
Mat::operator+=(eGlue) [done]
Mat::operator-=(eGlue) [done]
Mat::operator*=(eGlue) 
Mat::operator/=(eGlue) [done]

Mat::operator= (Glue) 
Mat::operator+=(Glue) 
Mat::operator-=(Glue) 
Mat::operator*=(Glue) 
Mat::operator/=(Glue) 

Mat::operator= (Op) 
Mat::operator+=(Op) 
Mat::operator-=(Op) 
Mat::operator*=(Op) 
Mat::operator/=(Op) 

Mat::submat() [done]

Mat::operator= (subview) [done]
Mat::operator+=(subview) [done]
Mat::operator-=(subview) [done]
Mat::operator*=(subview) 
Mat::operator/=(subview) [done]

.col()
subview_col

.row()
subview_row

.diag()
diagview

Mat::operator= (diagview)
Mat::operator+=(diagview)
Mat::operator-=(diagview)
Mat::operator*=(diagview)
Mat::operator/=(diagview)




.is_vec()     [done]
.is_colvec()  [done]
.is_rowvec()  [done]
.is_square()  [done]
.is_empty()   [done]


glue_times -> requires Glue


op_trans
trans() -> requires Op
.t()
.st()
.ht()

.min()
.max()
.index_min()
.index_max()

as_scalar() [done]
accu()      [done]  TODO: performance issues
trace()     [done]

sum(X,dim) [done]   TODO: performance issues
min(X,dim)
max(X,dim)

dot(A,B)  -> TODO: use clBLAS gemv() workaround

.zeros() [done]
.ones()  [done]
.eye()   [done]
standalone zeros()
standalone ones()
standalone eye()



randu()
randn()

all()
any()
abs()

norm()
repmat()

exp() [done]
log() [done]
clamp()
diagmat()
diagvec()
vectorise()



TODO: get a list of most used functions used by mlpack

zeros
ones
eye
shuffle
randu
randn
abs
sum
solve
mean
find
reshape
qr_econ
linspace
diagmat
trans
.t()
norm
subvec
randi
max
as_scalar
.col()
element initialisation
cube
exp
log
submat
sort_index
chol
vectorise
fliplr
fft2
ifft2
dot
join_cols
